# -*- coding: utf-8 -*-
"""loans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--QvXtSgS0q5-NerQ8qhZn5hXtUyAUSx

# Stout Case Study #1: Loans data

*First, we import useful libraries and load the data.*
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import matplotlib.image as mpimg


import warnings
warnings.filterwarnings("ignore")

import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

loans = pd.read_csv("/content/drive/MyDrive/Stout/loans_full_schema.csv")

"""## 1) Describe the dataset and any issues with it"""

loans.shape

"""The dataset contains 10,000 rows and describes 55 features."""

loans.head()

loans.info()

"""Here we have information on the distribution of each numerical variable: count, mean, std, min, max and quantiles. """

loans.describe()

"""Then we compute the percentage of missing values for each variable:"""

loans.isnull( ).sum( )/len(loans)*100

"""The most problematic features are:
- `verification_income_joint`(Type of verification of the joint income): 85.45% of missing values 
- `annual_income_joint` (If this is a joint application, then the annual income of the two parties applying.): 85.05% of missing values
- `debt_to_income_joint`(Debt-to-income ratio for the two parties.): 85.05% of missing values
- `months_since_90d_late`(Months since the last time the applicant was 90 days late on a payment.): 77.15% of missing values
- `months_since_last_delinq`(Months since the last delinquency.): 56.58% of missing values




We also have missing values for the following features, but at a lesser extent:
- `months_since_last_credit_inquiry`(Number of months since the last credit inquiry on this applicant.): 12.71% of missing values
- `emp_title`(Job title.): 8.33% of missing values
- `emp_length`(emp_length): 8.17% of missing values

The main reason why there are so many missing values for the features ending with 'joint' is that they concerns people who are not part of a joint account, so the data for these variable are not known. 
For the other missing values, it appears that the reason is also because the feature does not apply to the person that the row is describing. (e.g. for `months_since_last_credit_inquiry`, when the value is missing, it may be because this person has never experienced a credit inquiry.

Below is the number of unique values for each feature:
"""

loans.nunique( )

loans.emp_title.value_counts()

"""Overall, the dataset seems quite clean: no outlier points, just some missing values due to rows that are not all applicable on the corresponding feature. If we need to replace the missing values by others, we will do it next.

## 2)Generate a minimum of 5 unique visualizations using the data and write a brief description of your observations.

First is plotted the distribution of the `annual_income`:
"""

sns.histplot(data=loans,x="annual_income")

"""We can see on this histogram that the distribution of the annual income is right skewed.

Then is plotted the annual income against the interest rate for several combinations of loan status and application type..
"""

fg = sns.relplot(x="annual_income", y="interest_rate", data=loans, kind="scatter",
            hue="homeownership", size="grade", 
            col='application_type', row='loan_status', style='term',
            )

"""The main takeaway here is that for a small range of annual income, the interest rate is taking a wide range of values."""

jp = sns.jointplot("annual_income", "interest_rate", hue = "homeownership",data=loans)

"""It appears here that the people who own their house have a much more uniform distributino of the interest rate. """

cp1 = sns.catplot(x="homeownership", y="interest_rate", hue="loan_status", data=loans, kind="violin", 
            jitter=False, dodge=True)

"""In the violin plot above, we can see that for mortgage homeownership, the value of the interest rate is much more widely distributed than for rent and own status. Moreover, the "current" loan status, with "charged off", seem to have a bell shaped distribution of the interest rate, compared to the other status."""

cp2 = sns.catplot(x="public_record_bankrupt", y="interest_rate", hue="grade", data=loans, kind="bar",hue_order=["A","B","C","D","E","F","G"])

"""Finally, this bar plot clearly shows that the better the grade is, the lower the interest_rate here, regardless of the public_record_bankrupt.

## 3) Create a feature set and create a model which predicts interest rate using at least 2 algorithms. Describe any data cleansing that must be performed and analysis when examining the data.

### Feature selection

We divide the features between numerical and categorical.
We fill the missing values with 0 in order to keep the rows that contains this information.
"""

loans=loans.fillna(0)

first_column = loans.pop('interest_rate')

loans_numerical = loans.select_dtypes(include=['int64','float64'])
loans_numerical

loans_categorical = loans.select_dtypes(include="object")
loans_categorical_one_hot = pd.get_dummies(loans_categorical)
loans_categorical_one_hot

loans_train = pd.concat([loans_numerical,loans_categorical_one_hot],axis=1)

X_num = loans_numerical.values
X_cat = loans_categorical_one_hot.values
Y = first_column.values.astype('int')

"""For both types of features, we select the 20 best scores for the chi2 test.
Below are the names of the features kept for training, for both type of data.
"""

from sklearn.feature_selection import SelectKBest,chi2

Selector_numerical= SelectKBest(chi2, k=25)
X_num_selected=Selector_numerical.fit_transform(X_num, Y)
features_numerical = loans_numerical.columns.values[Selector_numerical.get_support()]
features_numerical

Selector_categorical= SelectKBest(chi2, k=25)
X_cat_selected=Selector_categorical.fit_transform(X_cat, Y)
features_categorical = loans_categorical_one_hot.columns.values[Selector_categorical.get_support()]
features_categorical

X=np.concatenate([X_num_selected,X_cat_selected],axis=1)
X.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X_train, y_train)
reg.score(X_test, y_test)

from sklearn.linear_model import Ridge
clf=Ridge(alpha=0.1).fit(X_train, y_train)
clf.score(X_test,y_test)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=200).fit(X_train, y_train)
rf.score(X_test, y_test)

"""Here we used three algorithms for predicting the interest rate:
- a Linear Regression model
- a Ridge Regression with $\lambda =0.1$ 
- a Random Forest Regressor, an ensemble algorithm with 200 estimators

The algorithm with the best score on the test data is the **Random Forest algorithm**, with a score of **99.49%** on the test data.

### 4) Visualize the test results and propose enhancements to the model, what would you do if you had more time. Also describe assumptions you made and your approach.
"""

import matplotlib.pyplot as plt
plt.plot(["Linear Regression","Ridge Regresison","Random Forest"],[reg.score(X_test, y_test),clf.score(X_test,y_test),rf.score(X_test, y_test)])

"""To enhance the model, I could think about other algorithms, such as neural networks (MultiLayer Perceptron) or even using cross-validation for finding a better lambda for Ridge Regression.

A major assumption was the use of the most inluential feature regarding the chi2 test. I could have used more features if I had more time.
Another assumption was the fact of replacing the missing values with 0. We could have dropped those rows for a better accuracy but I wanted to keep the information related to these rows.
"""

